{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dataset and PreProcessing for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtqiSMNM4E6A"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../..')))\n",
    "import urllib\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import namedtuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from random import seed, shuffle\n",
    "from datetime import date\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "\n",
    "### Adult dataset       \n",
    "def load_adult(scaler=True):\n",
    "    \n",
    "    data = pd.read_csv(\n",
    "        \"adult.data\",\n",
    "        names=[\n",
    "            \"Age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "            \"occupation\", \"relationship\", \"race\", \"gender\", \"capital gain\", \"capital loss\",\n",
    "            \"hours per week\", \"native-country\", \"income\"]\n",
    "            )\n",
    "    len_train = len(data.values[:, -1])\n",
    "    data_test = pd.read_csv(\n",
    "        \"adult.test\",\n",
    "        names=[\n",
    "            \"Age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "            \"occupation\", \"relationship\", \"race\", \"gender\", \"capital gain\", \"capital loss\",\n",
    "            \"hours per week\", \"native-country\", \"income\"],\n",
    "        skiprows=1, header=None\n",
    "    )\n",
    "    data = pd.concat([data, data_test])\n",
    "    # Considering the relative low portion of missing data, we discard rows with missing data\n",
    "    domanda = data[\"workclass\"][4].values[1] # domanda = ?\n",
    "    data = data[data[\"workclass\"] != domanda]\n",
    "    data = data[data[\"occupation\"] != domanda]\n",
    "    data = data[data[\"native-country\"] != domanda]\n",
    "\n",
    "    # Here we apply discretisation on column marital_status\n",
    "    data.replace(['Divorced', 'Married-AF-spouse',\n",
    "                  'Married-civ-spouse', 'Married-spouse-absent',\n",
    "                  'Never-married', 'Separated', 'Widowed'],\n",
    "                 ['not married', 'married', 'married', 'married',\n",
    "                  'not married', 'not married', 'not married'], inplace=True)\n",
    "    # categorical fields\n",
    "    category_col = ['workclass', 'race', 'education', 'marital-status', 'occupation',\n",
    "                    'relationship', 'gender', 'native-country', 'income']\n",
    "    for col in category_col:\n",
    "        b, c = np.unique(data[col], return_inverse=True)\n",
    "        data[col] = c\n",
    "    \n",
    "    datamat = data.values\n",
    "    \n",
    "    #Care there is a final dot in the class only in test set which creates 4 different classes\n",
    "    target = np.array([-1.0 if (val == 0 or val==1) else 1.0 for val in np.array(datamat)[:, -1]])\n",
    "    \n",
    "    datamat = datamat[:, :-1] # it delete last coloumn\n",
    "    \n",
    "    if scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(datamat)\n",
    "        datamat = scaler.transform(datamat)\n",
    "    \n",
    "    \n",
    "    nTrain = len_train\n",
    "    \n",
    "    data = namedtuple('_', 'data, target')(datamat[:nTrain, :], target[:nTrain])   # nTrain = 32561\n",
    "    data_test = namedtuple('_', 'data, target')(datamat[len_train:, :], target[len_train:])  #len_train = 32561\n",
    "    encoded_data = pd.DataFrame(data.data)\n",
    "    encoded_data['Target'] = (data.target+1)/2\n",
    "    to_protect = 1. * (data.data[:,8]!=data.data[:,8][0])\n",
    "    encoded_data_test = pd.DataFrame(data_test.data)\n",
    "    encoded_data_test['Target'] = (data_test.target+1)/2\n",
    "    to_protect_test = 1. * (data_test.data[:,8]!=data_test.data[:,8][0])\n",
    "    return encoded_data, encoded_data.drop(columns=8), encoded_data_test, encoded_data_test.drop(columns=8) # to_protect,, to_protect_test\n",
    "\n",
    "\n",
    "### COMPAS dataset\n",
    "def compute_num_days(s1,s2):\n",
    "    \n",
    "    date1 = date(int(s1[0:4]),int(s1[5:7]),int(s1[8:10]))\n",
    "    date2 = date(int(s2[0:4]),int(s2[5:7]),int(s2[8:10]))\n",
    "    delta = date2-date1\n",
    "    return delta.days\n",
    "\n",
    "def compute_days_series(data):\n",
    "    num_days = []\n",
    "    for i in range(data.shape[0]):\n",
    "        s1 = data[\"c_jail_in\"].iloc[i]\n",
    "        s2 = data[\"c_jail_out\"].iloc[i]\n",
    "        if type(s1) is str and type(s2) is str:\n",
    "            num_days = num_days + [compute_num_days(s1,s2)]\n",
    "        else:\n",
    "            num_days = num_days + [None]\n",
    "    return num_days\n",
    "\n",
    "def get_unique(data,att):\n",
    "    column_values = data[att].values.ravel()\n",
    "    unique_values =  pd.unique(column_values)\n",
    "    return unique_values\n",
    "\n",
    "#Encode categorical data to one-hot encoding scheme\n",
    "def enc_cat_attr(data2,col):\n",
    "    attr_values = get_unique(data2,col)\n",
    "    if len(attr_values) > 2:\n",
    "        for i in attr_values:\n",
    "            data2[col+'_'+str(i)] = data2[col] == i\n",
    "        data2 = data2.drop([col],axis=1)\n",
    "    else:\n",
    "        enc = preprocessing.LabelEncoder()\n",
    "        data2[col] = enc.fit_transform(data2[col])\n",
    "    data_cols = []\n",
    "    for i in data2.columns:\n",
    "        if i != 'two_year_recid':\n",
    "            data_cols = data_cols + [i]\n",
    "    data_cols = data_cols + ['two_year_recid']\n",
    "    return data2[data_cols]\n",
    "\n",
    "def load_compas():\n",
    "    # Load dataset from file\n",
    "    data = pd.read_csv(\n",
    "        \"compas-scores-two-years.csv\"\n",
    "            )\n",
    "    data_size = data.shape\n",
    "    data_nulls = data.isnull().sum()/data_size[0]\n",
    "    for i in data_nulls.keys():\n",
    "        if data_nulls[i] > 0.5:\n",
    "            data = data.drop(i,axis=1)\n",
    "    data = data.drop([\"decile_score\",\"score_text\",\"v_decile_score\",\"v_score_text\"],axis=1)\n",
    "    y_labels = data[\"two_year_recid\"]\n",
    "    data = data.drop([\"decile_score.1\",\"priors_count.1\",\"is_recid\"],axis=1)\n",
    "    data = data.drop([\"id\",\"name\",\"first\",\"last\",\"dob\",\"age\",\"c_case_number\",\"v_screening_date\", \"v_type_of_assessment\",\"type_of_assessment\",\n",
    "                  \"compas_screening_date\",\"days_b_screening_arrest\",\"is_violent_recid\",\"c_charge_desc\",\"screening_date\",\"in_custody\",\n",
    "                  \"out_custody\",\"start\",\"end\",\"event\",\"c_offense_date\",\"c_days_from_compas\"],axis=1)\n",
    "\n",
    "    data['num_jail_days'] = pd.Series(compute_days_series(data))\n",
    "    data = data.drop([\"c_jail_in\",\"c_jail_out\"],axis=1)\n",
    "    \n",
    "    #Filling in missing data for num_jail_days by averaging those with the same label class\n",
    "    mean_days_recid = int(data.loc[data['two_year_recid'] == 1].num_jail_days.mean())\n",
    "    mean_days_no_recid = int(data.loc[data['two_year_recid'] == 0].num_jail_days.mean())\n",
    "\n",
    "    data['num_jail_days'][(data['num_jail_days'].isnull()) & (data['two_year_recid'] == 1)] = mean_days_recid \n",
    "    data['num_jail_days'][(data['num_jail_days'].isnull()) & (data['two_year_recid'] == 0)] = mean_days_no_recid\n",
    "    attr_t = {'sex':'Male', 'race':'Caucasian', 'c_charge_degree':'M'}\n",
    "    for i in attr_t.keys():\n",
    "        data[i][data[i] != attr_t[i]] = 0\n",
    "        data[i][data[i] == attr_t[i]] = 1\n",
    "    #encoding 'priors_count' where n=0 is class 0, n=1,2 is class 1 and n>2 is class 2\n",
    "    data['priors_count'][data['priors_count']==0] = 0\n",
    "    data['priors_count'][(data['priors_count']>0) & (data['priors_count']<=2)] = 1\n",
    "    data['priors_count'][data['priors_count']>2] = 2\n",
    "\n",
    "    #encoding 'age_cat' where 'Less than 25' is class 0, '25 - 45' is class 1 and 'Greater than 45' is class 2\n",
    "    data['age_cat'][data['age_cat']=='Less than 25'] = 0\n",
    "    data['age_cat'][data['age_cat']=='25 - 45'] = 1\n",
    "    data['age_cat'][data['age_cat']=='Greater than 45'] = 2\n",
    "\n",
    "    #encoding 'num_jail_days' where n>=3 is class 0, n=3:13 is class 1 and n>13 is class 2\n",
    "    data['num_jail_days'][data['num_jail_days']<=3] = 0\n",
    "    data['num_jail_days'][(data['num_jail_days']>3) & (data['num_jail_days']<=13)] = 1\n",
    "    data['num_jail_days'][data['num_jail_days']>13] = 2\n",
    "\n",
    "    #encoding 'juv_fel_counts' where n=0 is class 0 and n>0 in class 1\n",
    "    data['juv_fel_count'][data['juv_fel_count']==0] = 0\n",
    "    data['juv_fel_count'][data['juv_fel_count']>0] = 1\n",
    "\n",
    "    #encoding 'juv_misd_counts' where n=0 is class 0 and n>0 in class 1\n",
    "    data['juv_misd_count'][data['juv_misd_count']==0] = 0\n",
    "    data['juv_misd_count'][data['juv_misd_count']>0] = 1\n",
    "\n",
    "    #encoding 'juv_other_counts' where n=0 is class 0 and n>0 in class 1\n",
    "    data['juv_other_count'][data['juv_other_count']==0] = 0\n",
    "    data['juv_other_count'][data['juv_other_count']>0] = 1\n",
    "\n",
    "    #one-hot encode the categorical attributes with cats > 2\n",
    "    data = enc_cat_attr(data,'priors_count')\n",
    "    data = enc_cat_attr(data,'num_jail_days')\n",
    "    data = enc_cat_attr(data,'age_cat')\n",
    "    #data = data.drop(['priors_count'],axis=1)\n",
    "\n",
    "    #replace boolean with float values\n",
    "    data = data.replace([True,False,1,0],[1.0,0.0,1.0,0.0])\n",
    "    \n",
    "    #convert dataset to numpy array\n",
    "    data_array = np.array(data)\n",
    "    x = data_array[:,:-1]\n",
    "    y = data_array[:,-1]\n",
    "    \n",
    "    #split data into train and test sets\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "    \n",
    "    data = namedtuple('_', 'data, target')(xtrain, ytrain)   # nTrain = 32561\n",
    "    data_test = namedtuple('_', 'data, target')(xtest, ytest)  #len_train = 32561\n",
    "    encoded_data = pd.DataFrame(data.data)\n",
    "    encoded_data['Target'] = data.target\n",
    "    encoded_data_test = pd.DataFrame(data_test.data)\n",
    "    encoded_data_test['Target'] = data_test.target\n",
    "    return encoded_data, encoded_data.drop(columns=1), encoded_data_test, encoded_data_test.drop(columns=1)\n",
    "\n",
    "\n",
    "### Law dataset\n",
    "def load_law_school():\n",
    "    \"\"\"Law school admission data.\n",
    "    The Law School Admission Council conducted a survey across 163 law schools in the United\n",
    "    States. It contains information on 21,790 law students such as their entrance exam scores\n",
    "    (LSAT), their grade-point average (GPA) collected prior to law school, and their first year\n",
    "    average grade (FYA).\n",
    "    Given this data, a school may wish to predict if an applicant will have a high FYA. The school\n",
    "    would also like to make sure these predictions are not biased by an individualâ€™s race and sex.\n",
    "    However, the LSAT, GPA, and FYA scores, may be biased due to social factors.\n",
    "    Example:\n",
    "        >>> import ethik\n",
    "        >>> X, y = ethik.datasets.load_law_school()\n",
    "        >>> X.head(10)\n",
    "                race     sex  LSAT  UGPA region_first  ZFYA  sander_index\n",
    "        0      White  Female  39.0   3.1           GL -0.98      0.782738\n",
    "        1      White  Female  36.0   3.0           GL  0.09      0.735714\n",
    "        2      White    Male  30.0   3.1           MS -0.35      0.670238\n",
    "        5   Hispanic    Male  39.0   2.2           NE  0.58      0.697024\n",
    "        6      White  Female  37.0   3.4           GL -1.26      0.786310\n",
    "        7      White  Female  30.5   3.6           GL  0.30      0.724107\n",
    "        8      White    Male  36.0   3.6           GL -0.10      0.792857\n",
    "        9      White    Male  37.0   2.7           NE -0.12      0.719643\n",
    "        13     White  Female  37.0   2.6           GL  1.53      0.710119\n",
    "        14     White    Male  31.0   3.6           GL  0.34      0.730357\n",
    "        >>> y.head(10)\n",
    "        0      True\n",
    "        1      True\n",
    "        2      True\n",
    "        5      True\n",
    "        6      True\n",
    "        7      True\n",
    "        8      True\n",
    "        9     False\n",
    "        13     True\n",
    "        14     True\n",
    "        Name: first_pf, dtype: bool\n",
    "    References:\n",
    "        1. https://papers.nips.cc/paper/6995-counterfactual-fairness.pdf\n",
    "        2. https://github.com/mkusner/counterfactual-fairness\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(\"law_data.csv\", dtype={\"race\": \"category\", \"region_first\": \"category\"},index_col=0)\n",
    "    X[\"sex\"] = X[\"sex\"].map(lambda z: 0 if z == 2 else 1)\n",
    "    X[\"race\"] = X[\"race\"].map(lambda z: 1 if z == \"White\" else 0)\n",
    "    X[\"region_first\"] = X[\"region_first\"].map(lambda z: 1 if z == \"GL\" else 0)\n",
    "    count_class_0, count_class_1 = X.first_pf.value_counts()\n",
    "\n",
    "    X_class_0 = X[X['first_pf'] == 0]\n",
    "    \n",
    "    X_class_1 = X[X['first_pf'] == 1]\n",
    "#     print(count_class_0)\n",
    "    X_class_1_under = X_class_1.sample(1*count_class_1)\n",
    "    X_under = pd.concat([X_class_1_under, X_class_0], axis=0)\n",
    "    y = X_under.pop(\"first_pf\").apply(int).astype(bool)\n",
    "    y = y.replace([True,False],[1.0,0.0])\n",
    "    \n",
    "    X = np.array(X_under)\n",
    "    y = np.array(y)\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    data = namedtuple('_', 'data, target')(xtrain, ytrain)   # nTrain = 32561\n",
    "    data_test = namedtuple('_', 'data, target')(xtest, ytest)\n",
    "    encoded_data = pd.DataFrame(data.data)\n",
    "    encoded_data['Target'] = data.target\n",
    "    encoded_data_test = pd.DataFrame(data_test.data)\n",
    "    encoded_data_test['Target'] = data_test.target\n",
    "    return  encoded_data, encoded_data.drop(columns=0), encoded_data_test, encoded_data_test.drop(columns=0)\n",
    "\n",
    "\n",
    "### Moon dataset\n",
    "def load_moon():\n",
    "    n_train = 10000\n",
    "    n_test = 5000\n",
    "    X, Y = make_moons(n_samples=n_train+n_test, noise=0.2, random_state=0)\n",
    "    Z = np.zeros_like(Y)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    for i in range(n_train + n_test):\n",
    "        if Y[i] == 0:\n",
    "            if -0.734 < X[i][0] < 0.734:\n",
    "                Z[i] = np.random.binomial(1, 0.90)\n",
    "            else:\n",
    "                Z[i] = np.random.binomial(1, 0.35)\n",
    "        elif Y[i] == 1:\n",
    "            if 0.262 < X[i][0] < 1.734:\n",
    "                Z[i] = np.random.binomial(1, 0.55)\n",
    "            else:\n",
    "                Z[i] = np.random.binomial(1, 0.10)\n",
    "    X = pd.DataFrame(X, columns=['x_1', 'x_2'])\n",
    "    Y = pd.Series(Y, name='label')\n",
    "    Z = pd.Series(Z, name='sensitive attribute')\n",
    "\n",
    "    X_train = X.loc[list(range(10000)), :]\n",
    "    Y_train = Y.loc[list(range(10000))]\n",
    "    Z_train = Z.loc[list(range(10000))]\n",
    "\n",
    "    X_test = X.loc[list(range(10000,15000)), :]\n",
    "    Y_test = Y.loc[list(range(10000,15000))]\n",
    "    Z_test = Z.loc[list(range(10000,15000))]\n",
    "\n",
    "    XZ_train = np.concatenate((X_train, np.reshape(Z_train.values,(-1,1))), axis = 1)\n",
    "    XZ_test = np.concatenate((X_test, np.reshape(Z_test.values,(-1,1))), axis = 1)\n",
    "   \n",
    "    return  X_train, XZ_train, Y_train, X_test, XZ_test, Y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enable GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fB_4M1_767-A",
    "outputId": "4bc5ed97-6d3b-4d51-8a15-2a2b6d184cb0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQt7Yoh55P5k",
    "outputId": "2add9c88-e7ab-41e6-cb6f-f4a0a9cc40ac"
   },
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('')))\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## classifier for CelebA dataset\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale=False):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        # modified layer4's stride to be 1 instead of 2 bc of smaller 64x64 size\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "        self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "### Define the classifier for low dimentional datasets\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        size = 200\n",
    "        self.first = nn.Linear(input_size, size)\n",
    "        self.last = nn.Linear(size, size)    \n",
    "        self.last2 = nn.Linear(size, 1)\n",
    "    def forward(self, x):\n",
    "        out = F.selu(self.first(x))\n",
    "        out = F.selu(self.last(out))\n",
    "        out = self.last2(out)\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "### Define the F-divergence estimator    \n",
    "class T_div(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T_div,self).__init__()\n",
    "        self.w_params = nn.Sequential (\n",
    "            nn.Linear(1,5),\n",
    "            nn.Sigmoid(),\n",
    "    \n",
    "            nn.Linear(5,1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_params(x)\n",
    "        return x\n",
    "\n",
    "### Training proccess\n",
    "result_lambda = pd.DataFrame()\n",
    "for lam in range(10):\n",
    "    n_seeds = 5\n",
    "    result_temp = pd.DataFrame()\n",
    "    for seed in range(n_seeds):\n",
    "        print('Currently working on - seed: {}'.format(seed))\n",
    "        # Set a seed for random number generation\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # Load dataset:\n",
    "        # Options for datasets are :\n",
    "        # 1:' COMPAS': load_compas()\n",
    "        # 2: 'Law School': load_law_school()\n",
    "        # 3:'Adult': load_adult()\n",
    "        # 4:'MOON': X_train, XZ_train, Y_train, X_test, XZ_test, Y_test=load_moon()\n",
    "        # 5:'CelebA': torch.load(\"filename\") # train .pt file / test .pt file \n",
    "        encoded_data_xz, encoded_data, encoded_data_test_xz, encoded_data_test=load_compas()\n",
    "        # Hyper Parameters\n",
    "        input_size = encoded_data.shape[1]-1 # for Moon: X_train.shape[1]\n",
    "        num_epochs = 200\n",
    "        batch_size = 2048\n",
    "        # Differernt datasets have different learning rate, please see supplementary material\n",
    "        learning_rate = 6e-4\n",
    "        cfg_factory=namedtuple('Config', 'model  batch_size num_epochs learning_rate input_size ' )\n",
    "        config = cfg_factory(Classifier, batch_size, num_epochs, learning_rate, input_size)\n",
    "        # Load classification model and F-divergence estimator\n",
    "        model = config.model(config.input_size).to(device)\n",
    "        T_x = T_div().to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        # Optimizer for classifier and F-divergence estimator\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=0)#1e-5\n",
    "        optimizer_T = torch.optim.Adam(T_x.parameters(), lr=config.learning_rate, weight_decay=0)\n",
    "        # Prepare real world datasets for training\n",
    "        train_target = torch.tensor(encoded_data['Target'].values.astype(np.long)).long().to(device)\n",
    "        train_data = torch.tensor(encoded_data.drop('Target', axis = 1).values.astype(np.float32)).to(device)\n",
    "        train_protect = torch.tensor(encoded_data_xz.drop('Target', axis = 1).values.astype(np.float32)).to(device)\n",
    "        train_tensor = data_utils.TensorDataset(train_data, train_protect, train_target)\n",
    "        train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = config.batch_size, shuffle = True)\n",
    "        # Prepare Moon for training\n",
    "        # train_target = torch.tensor(Y_train.values.astype(np.long)).long().to(device)\n",
    "        # train_data = torch.tensor(X_train.values.astype(np.float32)).to(device)\n",
    "        # train_protect = torch.tensor(XZ_train.astype(np.float32)).to(device)\n",
    "        # train_tensor = data_utils.TensorDataset(train_data,train_protect, train_target)\n",
    "        # train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = config.batch_size, shuffle = True)\n",
    "\n",
    "        # Prepare real world datasets for testing\n",
    "        target = torch.tensor(encoded_data_test['Target'].values.astype(np.long)).long().to(device)\n",
    "        data = torch.tensor(encoded_data_test.drop('Target', axis = 1).values.astype(np.float32)).to(device)\n",
    "        test_data_xz = torch.tensor(encoded_data_test_xz.drop('Target', axis = 1).values.astype(np.float32)).to(device)\n",
    "        # Prepare Moon for testing\n",
    "        # target = torch.tensor(Y_test.values.astype(np.long)).long().to(device)\n",
    "        # data = torch.tensor(X_test.values.astype(np.float32)).to(device)\n",
    "        # test_data_xz = torch.tensor(XZ_test.astype(np.float32)).to(device)\n",
    "\n",
    "\n",
    "        # Function to calculate classification accuracy\n",
    "        def calc_accuracy(outputs,Y): \n",
    "            outputs = (outputs >= 0.5).float()\n",
    "            acc = (outputs == Y).sum().float()/len(Y)\n",
    "            return acc\n",
    "        \n",
    "        # Initialize the value\n",
    "        epoch_vector = []\n",
    "\n",
    "        lamda = lam\n",
    "        di_list = []\n",
    "        deo_list = []\n",
    "        test_loss_list = []\n",
    "        test_accuracy = []\n",
    "        acc_mean_vector = []\n",
    "        r2_mean_vector = []\n",
    "        loss_mean_vector = []\n",
    "        divergence=0\n",
    "        loss_regu = 0\n",
    "        loss = 0\n",
    "        # Start Training\n",
    "        for epoch in range(config.num_epochs):\n",
    "            for i, (x, xz, y) in enumerate(train_loader):\n",
    "                # Update F-divergence Estimator\n",
    "                # Notice that for Moon, Adult, COMPAS dataset, F-divergence estimator needs to update j=100 steps;\n",
    "                # for adult dataset, it needs j=10 steps.\n",
    "                for j in range(100):\n",
    "                    T_x.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    \n",
    "                    \n",
    "                    # Here we separate the outputs in two groups. \n",
    "                    # For different dataset the index for sensitive attributs are different, which can refer from dataset.\n",
    "                    # The sensitive attribute for COMPAS dataset is 1, i.e., xz[:,1].\n",
    "                    Tx_output_xz0 = T_x(outputs[xz[:,1]==0])\n",
    "                    Tx_output_xz1 = T_x(outputs[xz[:,1]!=0])\n",
    "                    \n",
    "                    \n",
    "                    # Make the numbers of the two groups equally\n",
    "                    min_index = min(len(Tx_output_xz0),len(Tx_output_xz1))\n",
    "                    Tx_output_xz0_index = torch.randperm(len(Tx_output_xz0))[:min_index]\n",
    "                    Tx_output_xz1_index = torch.randperm(len(Tx_output_xz1))[:min_index]\n",
    "                    Tx_output_xz0 = Tx_output_xz0[Tx_output_xz0_index]\n",
    "                    Tx_output_xz1 = Tx_output_xz1[Tx_output_xz1_index]\n",
    "                    \n",
    "                    # NN based F-divergence estimator\n",
    "                    # For KL divergence\n",
    "                    mean_P = torch.mean(Tx_output_xz0)\n",
    "                    mean_Q = torch.mean(torch.exp(Tx_output_xz1-1))\n",
    "                    ## For Pearson divergence\n",
    "                    # mean_P = torch.mean(Tx_output_xz0)\n",
    "                    # mean_Q = torch.mean((0.25*Tx_output_xz1**2+Tx_output_xz1))\n",
    "                    ## For SH divergence\n",
    "                    # mean_P = torch.mean(Tx_output_xz0)\n",
    "                    # mean_Q = torch.mean(Tx_output_xz1/(torch.tensor(1.)-Tx_output_xz1))\n",
    "                    \n",
    "                    \n",
    "                    # DRE F-divergence estimator (same for three divergences)\n",
    "                    # mean_P = torch.mean(torch.log(Tx_output_xz0+0.0000001))\n",
    "                    # mean_Q = torch.mean(Tx_output_xz1)-1\n",
    "                    \n",
    "                    \n",
    "                    # Backward process\n",
    "                    divergence_estimate = mean_P - mean_Q\n",
    "                    loss_div = -divergence_estimate\n",
    "                    loss_div.backward()\n",
    "                    optimizer_T.step()\n",
    "\n",
    "\n",
    "                # Update Classifier\n",
    "                model.zero_grad()\n",
    "                outputs = model(x)\n",
    "                #Compute the loss of the classification\n",
    "                loss = criterion(outputs, torch.unsqueeze(y,dim=1).float())\n",
    "                \n",
    "                # Here we separate the outputs in two groups. \n",
    "                # For different dataset the index for sensitive attributs are different, which can refer from dataset.\n",
    "                # The sensitive attribute for COMPAS dataset is 1, i.e., xz[:,1].\n",
    "                outputs_xz0 = outputs[xz[:,1]==0]\n",
    "                outputs_xz1 = outputs[xz[:,1]!=0]\n",
    "                Tx_output_xz0 = T_x(outputs_xz0)\n",
    "                Tx_output_xz1 = T_x(outputs_xz1)\n",
    "                \n",
    "                # Make the numbers of the two groups equally\n",
    "                min_index = min(len(Tx_output_xz0),len(Tx_output_xz1))\n",
    "                Tx_output_xz0_index = torch.randperm(len(Tx_output_xz0))[:min_index]\n",
    "                Tx_output_xz1_index = torch.randperm(len(Tx_output_xz1))[:min_index]\n",
    "                Tx_output_xz0 = Tx_output_xz0[Tx_output_xz0_index]\n",
    "                Tx_output_xz1 = Tx_output_xz1[Tx_output_xz1_index]\n",
    "\n",
    "                # Convention method: If we use CON method, we do not use F-divergence estimator. We can directly add regularization term in the loss function\n",
    "                # You may comment F-divergence estimator training process to accelerate training process\n",
    "                # y0z0 = torch.mean(outputs_xz0)\n",
    "                # y0z1 = torch.mean(outputs_xz1)\n",
    "                # y1z0 = torch.mean(1-outputs_xz0)\n",
    "                # y1z1 = torch.mean(1-outputs_xz1)\n",
    "                # # For KL divergence\n",
    "                # divergence = y0z0*torch.log(y0z0/y0z1)+y1z0*torch.log(y1z0/y1z1)\n",
    "                # # For Pearson divergence\n",
    "                # divergence = (y0z0-y0z1)**2/y0z1+(y1z0-y1z1)**2/y1z1\n",
    "                # # For SH divergence\n",
    "                # divergence = (pow(y0z0,0.5)-pow(y0z1,0.5))**2+(pow(y1z0,0.5)-pow(y1z1,0.5))**2\n",
    "                \n",
    "\n",
    "                # NN based F-divergence estimator\n",
    "                \n",
    "                mean_P = torch.mean(Tx_output_xz0)\n",
    "                mean_Q = torch.mean(torch.exp(Tx_output_xz1-1))\n",
    "                ## For Pearson divergence\n",
    "                # mean_P = torch.mean(Tx_output_xz0)\n",
    "                # mean_Q = torch.mean((0.25*Tx_output_xz1**2+Tx_output_xz1))\n",
    "                ## For SH divergence\n",
    "                # mean_P = torch.mean(Tx_output_xz0)\n",
    "                # mean_Q = torch.mean(Tx_output_xz1/(torch.tensor(1.)-Tx_output_xz1))\n",
    "\n",
    "                # DRE method\n",
    "                # For KL divergence\n",
    "                # mean_P = torch.mean(torch.log(Tx_output_xz0+0.0000001))\n",
    "                ## For Pearson divergence\n",
    "                # mean_Q = torch.mean((Tx_output_xz1-1)**2)\n",
    "                ## For SH divergence        \n",
    "                # mean_Q = torch.mean((pow(Tx_output_xz1,0.5)-1)**2)\n",
    "                \n",
    "                divergence_estimate = abs(mean_P - mean_Q) # abs(mean_P) # abs(mean_Q)\n",
    "                loss_regu = torch.mean(divergence_estimate)       \n",
    "                # Regularization Loss w.r.t NN and DRE based estimator is lamda*loss_regu\n",
    "                # Regularization Loss w.r.t Convention estimator is lamda*divergence\n",
    "                loss = loss + lamda*loss_regu\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_mean_vector.append(divergence_estimate)\n",
    "                acc_mean_vector.append(calc_accuracy(outputs,torch.unsqueeze(y,dim=1)))\n",
    "                epoch_vector.append(epoch)\n",
    "\n",
    "        # Testing process\n",
    "        print(lamda)\n",
    "        print(\"Results on test set\")\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(data).detach()\n",
    "            loss = criterion(test_outputs, torch.unsqueeze(target,dim=1).float())\n",
    "            # Fairness Measurements\n",
    "            z0y1 = (torch.reshape((test_data_xz[:,1] == 0),(len((test_data_xz[:,1] == 0)),1)) *  (test_outputs >= 0.5)).sum().float() / (test_data_xz[:,1] == 0).sum().float()\n",
    "            z1y1 = (torch.reshape((test_data_xz[:,1] != 0),(len((test_data_xz[:,1] != 0)),1)) *  (test_outputs >= 0.5)).sum().float() / (test_data_xz[:,1] == 1).sum().float()\n",
    "            di_1 = z0y1/z1y1\n",
    "            if di_1 >= 1:\n",
    "                di_1 = 1/di_1\n",
    "            deo= abs(z1y1-z0y1)\n",
    "            result_temp = result_temp.append({\"Accuracy\":calc_accuracy(test_outputs.cpu(),torch.unsqueeze(target.cpu(),dim=1)),\"Diff\":deo.cpu(),\"Ratio\":di_1.cpu()},ignore_index=True)\n",
    "            print('Accuracy: %.4f, Ratio: %.4f, Diff: %.4f' % (calc_accuracy(test_outputs,torch.unsqueeze(target,dim=1)),di_1,deo))\n",
    "    result_lambda = result_lambda.append(result_temp.mean(),ignore_index=True)\n",
    "print(result_lambda)\n",
    "result_lambda.to_csv(\"result_compas_dp_kl.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "COMPAS_dp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
